{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow\n",
    "\n",
    "Basic TensorFlow linear regression notes\n",
    "\n",
    "This is a simple notebook to confirm my understanding on how TensorFlow works. I will do simple example with a univariate linear regression with numerical derivations. \n",
    "\n",
    "### Hypothesis function, h(x)\n",
    "y = mx\n",
    "\n",
    "\n",
    "\n",
    "### Starting parameter / Initial hypothesis \n",
    "m = 0 <br>\n",
    "\n",
    "\n",
    "### Target Parameter / Goal\n",
    "m = 1\n",
    "\n",
    "### Dataset \n",
    "(-3,3), (-2,-2), (-1, -1), (0,0), (1,1), (2,2), (3,3)\n",
    "\n",
    "### Loss function\n",
    "Square loss function<br>\n",
    "\n",
    "### Optimizer \n",
    "Gradient descent with $\\alpha = 0.0001 $\n",
    "\n",
    "\n",
    "We will also stop after one epoch and compare our calculated weight update with TensorFlow's to confirm our expectation. \n",
    "\n",
    "Note: This is using TensorFlow 1.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3, -2, -1, 0, 1, 2, 3]\n",
      "7\n",
      "loss val is... \n",
      "[9. 4. 1. 0. 1. 4. 9.]\n",
      "update w is..\n",
      "[0.00055999996]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "dataset_size = 6\n",
    "data = [i for i in range(-dataset_size//2,dataset_size//2+1)]\n",
    "print (data)\n",
    "print (len(data))\n",
    "X = tf.placeholder(tf.float32, name = \"X\")\n",
    "Y = tf.placeholder(tf.float32, name = \"Y\")\n",
    "\n",
    "# where we init our model with m = 0\n",
    "w = tf.Variable(0.0, name = \"W\")\n",
    "\n",
    "Y_prediction = X * w \n",
    "\n",
    "loss = tf.square(Y-Y_prediction, name = \"loss\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.00001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # run through one epoch \n",
    "    loss_val, grads = sess.run([loss , optimizer], feed_dict = {X: data, Y: data})\n",
    "    print (\"loss val is... \")\n",
    "    print (loss_val)\n",
    "    w_value = sess.run([w])\n",
    "    print (\"update w is..\")\n",
    "    print ( w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where did that ~0.00056 come from? Here are the steps:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial h} \\frac{\\partial h}{\\partial w}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where C is the Cost function, h is our hypothesis function, and w is our weight. These equations become.... \n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial h} = 2 (h(x) - y)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial h}{\\partial w} = y\n",
    "\\end{equation}\n",
    "\n",
    "so now... \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w} = 2 (h(x) - y) \\times y\n",
    "\\end{equation}\n",
    "\n",
    "At the first epoch, at $x = -3, y = -3$\n",
    "\n",
    "Our hypothesis is 0 at all times so $h(x) = h(-3) =  0$\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w} = 2 (h(x) - y) \\times \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " = 2 ( 0 - (-3)) \\times (-3)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " = -18\n",
    "\\end{equation}\n",
    "\n",
    "if we do this for the rest of the data points ( $x = -2, -1, 0 , 1, 2, -3 $ ) and sum them up, we will have... \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum \\frac{\\partial C}{\\partial w} = -18 + -4 + -2 + 0 + -2 + -4 + -18 = -56\n",
    "\\end{equation}\n",
    "\n",
    "Afterwards we follow the update rule:\n",
    "$$\n",
    "w_{initial} = 0\n",
    "$$\n",
    "\\begin{equation}\n",
    "w := w - \\alpha \\times \\frac{\\partial C}{\\partial w} = 0 - 0.0001 * -56 = 0.00056\n",
    "\\end{equation}\n",
    "\n",
    "$$\n",
    "w = 0.00056\n",
    "$$\n",
    "\n",
    "Now this is for one epoch. What happens if we increase it to 100,000 epochs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [0.9963077]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # run through one epoch \n",
    "    for i in range(10000):\n",
    "        loss_val, grads = sess.run([loss , optimizer], feed_dict = {X: data, Y: data})\n",
    "    w_value = sess.run([w]) \n",
    "    print ('w: ', w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that over time, given enough epochs, it'll get closer and closer to 1 which is exactly what we expect! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
